{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download datasets from https://grouplens.org/datasets/movielens/latest/\n",
    "# put them in a folder named \"data\"\n",
    "\n",
    "# make a movie index map\n",
    "movie_map = {} # dictionary\n",
    "with open(\"data/movies.csv\") as fp:\n",
    "    reader = csv.reader(fp)\n",
    "    header = next(fp)\n",
    "    for i, row in enumerate(reader):\n",
    "        movie_map[row[0]] = {\"idx\": i,\n",
    "                            \"title\": row[1],\n",
    "                            \"genres\": row[2].split(\"|\")}\n",
    "movie_remap = {v[\"idx\"]:k for k, v in movie_map.items()}\n",
    "\n",
    "# make a user index map\n",
    "user_map = {}\n",
    "with open(\"data/ratings.csv\") as fp:\n",
    "    reader = csv.reader(fp)\n",
    "    header = next(fp)\n",
    "    for i, row in enumerate(reader):\n",
    "        user_map[row[0]] = {\"idx\": int(row[0]) -1}\n",
    "user_remap = {v[\"idx\"]:k for k, v in user_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(user_remap)\n",
    "m = len(movie_remap)\n",
    "X = np.zeros((n, m))\n",
    "\n",
    "with open(\"data/ratings.csv\") as fp:\n",
    "    reader = csv.reader(fp)\n",
    "    header = next(fp)\n",
    "    for row in reader:\n",
    "        i = user_map[row[0]][\"idx\"]\n",
    "        j = movie_map[row[1]][\"idx\"]\n",
    "        v = float(row[2])\n",
    "        X[i,j] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9742)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Forrest Gump (1994)',\n",
       " 'Shawshank Redemption, The (1994)',\n",
       " 'Pulp Fiction (1994)',\n",
       " 'Silence of the Lambs, The (1991)',\n",
       " 'Matrix, The (1999)',\n",
       " 'Star Wars: Episode IV - A New Hope (1977)',\n",
       " 'Jurassic Park (1993)',\n",
       " 'Braveheart (1995)',\n",
       " 'Terminator 2: Judgment Day (1991)',\n",
       " \"Schindler's List (1993)\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Movies that have most ratings (not high/low, just the number of ratings)\n",
    "[movie_map[movie_remap[i]][\"title\"] for i in np.argsort(np.mean(X == 0, axis=0))[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I Know Where I'm Going! (1945)\",\n",
       " 'Twentieth Century (1934)',\n",
       " 'Road Home, The (Wo de fu qin mu qin) (1999)',\n",
       " 'This Gun for Hire (1942)',\n",
       " 'Chosen, The (1981)',\n",
       " 'Scrooge (1970)',\n",
       " 'Chalet Girl (2011)',\n",
       " 'Roaring Twenties, The (1939)',\n",
       " 'For All Mankind (1989)',\n",
       " 'Proof (1991)']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Movies that have least ratings (not high/low, just the number of ratings)\n",
    "[movie_map[movie_remap[i]][\"title\"] for i in np.argsort(np.mean(X > 0, axis=0))[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14045852139735537"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.mean((x-y)**2)\n",
    "\n",
    "D = 10\n",
    "svd = TruncatedSVD(n_components=D, n_iter=10, random_state=42)\n",
    "svd.fit(X)\n",
    "V = svd.components_ # V\n",
    "U = svd.transform(X) # U\n",
    "# X = U V + errors\n",
    "# X_est = U V (low-dimensional approximation of X)\n",
    "X_est = np.dot(U, V)\n",
    "mse(X, X_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.04498985e-02,  3.85393459e-02,  1.59129220e-02, ...,\n",
       "         6.46836073e-05,  6.46836073e-05,  2.71729303e-04],\n",
       "       [-2.75911951e-02, -2.06662709e-03, -2.47146155e-02, ...,\n",
       "         5.97586247e-04,  5.97586247e-04,  1.27236200e-03],\n",
       "       [-7.84438822e-02, -5.68447077e-02, -1.80051169e-02, ...,\n",
       "         8.71093840e-05,  8.71093840e-05, -1.22833422e-04],\n",
       "       ...,\n",
       "       [-1.80087832e-02, -3.11530214e-02, -4.42156282e-03, ...,\n",
       "         5.08518234e-04,  5.08518234e-04,  3.91830773e-04],\n",
       "       [-9.68990262e-03,  2.79837235e-02, -2.69464384e-02, ...,\n",
       "        -6.81949591e-05, -6.81949591e-05, -6.38930036e-04],\n",
       "       [-1.23218103e-02, -2.15380330e-03, -1.67550429e-02, ...,\n",
       "         3.57390691e-04,  3.57390691e-04, -1.18709148e-03]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.dot(U, V) + U0 (user bias) + V0 (movie bias) + error \n",
    "\n",
    "Z_est = np.dot(U, V)\n",
    "U0 = np.mean(X - Z_est, axis=1)\n",
    "V0 = np.mean(((X - Z_est).T - U0).T, axis=0)\n",
    "for i in range(30):\n",
    "    Z = (X.T - U0).T - V0\n",
    "    svd.fit(Z)\n",
    "    V = svd.components_ # V\n",
    "    U = svd.transform(Z) # U\n",
    "    Z_est = np.dot(U, V)\n",
    "    U0 = np.mean(X - Z_est - V0, axis=1)\n",
    "    V0 = np.mean(((X - Z_est).T - U0).T, axis=0)\n",
    "    X_est = ((Z_est + V0).T + U0).T\n",
    "    print(\"{}th iteration: {}\".format(i, mse(X, X_est)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(D):\n",
    "    print(\"== {}th component ==\".format(d))\n",
    "    print([(movie_map[movie_remap[i]][\"title\"], V[d,i]) for i in np.argsort(-V[d,:])[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=D, random_state=42)\n",
    "nmf.fit(X)\n",
    "V = nmf.components_ # V\n",
    "U = nmf.transform(X) # U\n",
    "X_est = np.dot(U, V)\n",
    "mse(X, X_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(D):\n",
    "    print(\"== {}th component ==\".format(d))\n",
    "    print([(movie_map[movie_remap[i]][\"title\"], V[d,i]) for i in np.argsort(-V[d,:])[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(D):\n",
    "    print(\"== {}th component ==\".format(d))\n",
    "    nf = 1/np.sum(V[d,:])*10000 # normalization factor, multiplying 10000 for readability no other reason...\n",
    "    print([(movie_map[movie_remap[i]][\"title\"], int(V[d,i]*nf)) for i in np.argsort(-V[d,:])[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=D, alpha=10.0, l1_ratio=1, random_state=42)\n",
    "nmf.fit(X)\n",
    "V = nmf.components_ # V\n",
    "U = nmf.transform(X) # U\n",
    "X_est = np.dot(U, V)\n",
    "print(mse(X, X_est))\n",
    "for d in range(D):\n",
    "    print(\"== {}th component ==\".format(d))\n",
    "    nf = 1/np.sum(V[d,:])*10000 # normalization factor, multiplying 10000 for readability no other reason...\n",
    "    print([(movie_map[movie_remap[i]][\"title\"], int(V[d,i]*nf)) for i in np.argsort(-V[d,:])[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10): # first 10 users\n",
    "    print(\"== {}th user ==\".format(d))\n",
    "    nf = 1/np.sum(U[i,:]) # normalization factor\n",
    "    print(U[i,:]*nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.dot(U, V) + U0 (user bias) + V0 (movie bias) + error \n",
    "# where U and V are non-negative\n",
    "Z_est = np.dot(U, V)\n",
    "U0 = np.mean(X - Z_est, axis=1)\n",
    "V0 = np.mean(((X - Z_est).T - U0).T, axis=0)\n",
    "for i in range(30):\n",
    "    Z = (X.T - U0).T - V0\n",
    "    Z[Z<0] = 0\n",
    "    nmf.fit(Z)\n",
    "    V = nmf.components_ # V\n",
    "    U = nmf.transform(Z) # U\n",
    "    Z_est = np.dot(U, V)\n",
    "    U0 = np.mean(X - Z_est - V0, axis=1)\n",
    "    V0 = np.mean(((X - Z_est).T - U0).T, axis=0)\n",
    "    X_est = ((Z_est + V0).T + U0).T\n",
    "    print(\"{}th iteration: {}\".format(i, mse(X, X_est)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(D):\n",
    "    print(\"== {}th component ==\".format(d))\n",
    "    nf = 1/np.sum(V[d,:])*10000 # normalization factor, multiplying 10000 for readability no other reason...\n",
    "    print([(movie_map[movie_remap[i]][\"title\"], int(V[d,i]*nf)) for i in np.argsort(-V[d,:])[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some prediction tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "# Linear Regression\n",
    "# Using the reduced features\n",
    "# Using the Matrix Completion\n",
    "# Netflix Competition..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_target = np.argmax(np.mean(X > 0, axis=0))\n",
    "movie_map[movie_remap[j_target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X[:,j_target]\n",
    "Z = np.hstack((X[:,:j_target], X[:,(j_target+1):]))\n",
    "has_rating = y>0\n",
    "y = y[has_rating]\n",
    "Z = Z[has_rating,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Z_train, Z_test, y_train, y_test = train_test_split(Z, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model_tree = DecisionTreeRegressor()\n",
    "model_enet = ElasticNet()\n",
    "\n",
    "model_tree.fit(Z_train, y_train)\n",
    "model_enet.fit(Z_train, y_train)\n",
    "\n",
    "y_pred_base = np.mean(y_train)\n",
    "y_pred_tree = model_tree.predict(Z_test)\n",
    "y_pred_enet = model_enet.predict(Z_test)\n",
    "\n",
    "print(\"mse(base): {}\".format(mse(y_test, y_pred_base)))\n",
    "print(\"mse(tree): {}\".format(mse(y_test, y_pred_tree)))\n",
    "print(\"mse(enet): {}\".format(mse(y_test, y_pred_enet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree = DecisionTreeRegressor(max_depth=2, min_samples_leaf=30)\n",
    "model_enet = ElasticNet(alpha=0.1, l1_ratio=1.0) # change alpha up/down\n",
    "\n",
    "model_tree.fit(Z_train, y_train)\n",
    "model_enet.fit(Z_train, y_train)\n",
    "\n",
    "y_pred_base = np.mean(y_train)\n",
    "y_pred_tree = model_tree.predict(Z_test)\n",
    "y_pred_enet = model_enet.predict(Z_test)\n",
    "\n",
    "print(\"mse(base): {}\".format(mse(y_test, y_pred_base)))\n",
    "print(\"mse(tree): {}\".format(mse(y_test, y_pred_tree)))\n",
    "print(\"mse(enet): {}\".format(mse(y_test, y_pred_enet)))\n",
    "print(model_enet.coef_[np.abs(model_enet.coef_) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 5\n",
    "nmf = NMF(n_components=D, alpha=10.0, l1_ratio=1, random_state=42)\n",
    "nmf.fit(Z_train)\n",
    "V = nmf.components_ # V\n",
    "U_train = nmf.transform(Z_train) \n",
    "U_test = nmf.transform(Z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree = DecisionTreeRegressor(max_depth=3, min_samples_leaf=30)\n",
    "model_enet = ElasticNet(alpha=0.01, l1_ratio=0.5) # change alpha up/down\n",
    "\n",
    "model_tree.fit(U_train, y_train)\n",
    "model_enet.fit(U_train, y_train)\n",
    "\n",
    "y_pred_base = np.mean(y_train)\n",
    "y_pred_tree = model_tree.predict(U_test)\n",
    "y_pred_enet = model_enet.predict(U_test)\n",
    "\n",
    "print(\"mse(base): {}\".format(mse(y_test, y_pred_base)))\n",
    "print(\"mse(tree): {}\".format(mse(y_test, y_pred_tree)))\n",
    "print(\"mse(enet): {}\".format(mse(y_test, y_pred_enet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "plot_tree(model_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(D):\n",
    "    print(\"== {}th component ==\".format(d))\n",
    "    nf = 1/np.sum(V[d,:])*10000 # normalization factor, multiplying 10000 for readability no other reason...\n",
    "    print([(movie_map[movie_remap[i]][\"title\"], int(V[d,i]*nf)) for i in np.argsort(-V[d,:])[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-shot prediction, for unseen movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "y_test = X_test[:,j_target].copy()\n",
    "X_test[:,j_target] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 5\n",
    "nmf = NMF(n_components=D, alpha=10.0, l1_ratio=1, random_state=42)\n",
    "nmf.fit(X_train)\n",
    "V = nmf.components_ # V\n",
    "U_train = nmf.transform(X_train) \n",
    "U_test = nmf.transform(X_test)\n",
    "y_pred_oneshot = np.dot(U_test, V)[:,j_target]\n",
    "print(\"mse(one-shot): {}\".format(mse(y_test, y_pred_oneshot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred_oneshot}).plot.scatter(x=\"y_true\", y=\"y_pred\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
